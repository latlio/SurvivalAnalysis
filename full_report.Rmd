---
title: "Cox PH Model and Something New"
author: "Madison Hobbs & Lathan Liou"
class: "MATH150: Methods in Biostatistics"
date: "4/19/2019"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyverse)
library(survival)
library(survminer)
library(rms)
library(glmnet)
library(broom)
library(xtable)
library(GGally)
data <- read_csv("AIDSdata.csv")
aids <- data

#parse as factor
data <- data %>%
  mutate(tx = as.factor(tx),
         time = as.numeric(time),
         txgrp = as.factor(txgrp),
         strat2 = as.factor(strat2),
         sex = as.factor(sex),
         raceth = as.factor(raceth),
         ivdrug = as.factor(ivdrug),
         hemophil = as.factor(hemophil),
         karnof = as.factor(karnof))
```

# Introduction

Our project seeks to understand time of survival until an AIDS defining event or death. We first seek to understand the distributions of our time-to-event variables and the remaining explanatory variables through some exploratory data analysis. Next, we try a series of Cox Proportional Hazards models. Finally, we investigate extentions including Bootstrapping techniques and alternative model algorithms such as XGBoost.

# Exploratory Data Analysis

## A Note About Treatments

According to the variable information table, we note that `txgrp` could have four levels (1: ZDV + 3TC, 2: ZDV + 3TC + IDV, 3: d4T + 3TC, and 4: d4T + 3TC + IDV). However, this dataset contains only two levels of `txgrp` (1: ZDV + 3TC, 2: ZDV + 3TC + IDV), as shown below:

```{r}
aids %>% group_by(txgrp) %>% summarise(n())
```

In fact, since the variable `tx` is supposed to indicate whether the treatment contained IDV, we might assume that `txgrp` and `tx` are redundant information in this dataset and that a 1 in `txgrp` is equivalent to a 0 in `tx` while a 2 in `txgrp` is equivalent to a 1 in `tx`. We confirm this hunch below. 

The following code says: create a new dataframe by taking all the rows in `data` where `txgrp` is 1 and `tx` is 0 *or* `txgrp` is 2 and `tx` is 1. Now, make sure that new dataframe is identical to the original data frame, and return `TRUE` if this is indeed the case. 


```{r}
# Is it true that for every entry in `aids`
all(
  (aids %>% 
     filter(((txgrp == 1 && tx == 0) || (txgrp == 2 && tx == 1)))) 
  == aids) == TRUE
```

## Correlation

We present a pairs plot of our explanatory variables, excluding id, our time-to-event variables, and our censoring variables to 1) visualize the distribution of the variables and 2) identify potential pair-wise correlation. \texttt{cd4} and \texttt{strat2} have a correlation coefficient of 0.74, which indicates moderate to strong correlation. This makes sense since \texttt{strat2} is the indicator variable for the continuous variable, \texttt{cd4}. Additionally, as noted just above, \texttt{tx} and \texttt{txgrp} provide the exact same information and therefore are perfectly correlated. Lastly, we would like to note that \texttt{sex}, \texttt{ivdrug}, and \texttt{hemophil} are highly unbalanced variables, meaning that one level of the variables is disproportionately represented relative to the other level(s).

```{r, fig.height=8, fig.width=13}
aids %>% select(tx:age) %>% ggpairs()
```

## Censored vs. Non-Censored

It's worth noting that there are, in fact, two censored time-to-event variables. The primary variable of interest is `time` which is time in days to AIDs diagnosis or death, and this is informed by `censor`, which is 1 (true) if an individual was either diagnosed with AIDS *or* died during the course of the study and 0 otherwise. The other censored variable is `time_d` which is the time in days to death alone, governed by `censor_d` which is 1 if the person died during the study and 0 if not. 

Since the primary variable of interest is time to AIDs diagnosis or death, we examine the complete (non-censored) individuals - those who were either diagnosed with AIDS *or* who died over the course of the study. The only caveat is that there are only 69 such individuals out of a study of 851 - most of the participants did not die or get diagnosed before the study's end. 

```{r}
non_censored <- aids %>% filter(censor == 1) %>% 
  mutate(tx=ifelse(tx == 0, "Control", "IDV"))
dim(non_censored) # complete AIDS or death
dim(aids)         # everyone
```

Among those with complete times, we notice from the side-by-side histograms below that both the control and IDV groups are skewed right. This makes sense - for complete observations, it's probably less common for people to last a long time without being diagnosed or dying. The distributions between the control and IDV groups don't look that different however, especially given the tiny sample sizes.

```{r}
ggplot(non_censored, aes(x = time)) + 
  geom_histogram(bins = 30) + 
  facet_grid(.~tx) + ggtitle("Complete Observations: Treatment vs. Time to Event") + 
  xlab("Time until AIDS Diagnosis or Death")
```
 
When looking at the censored (incomplete) times for diagnosis/death, both control and IDV groups are in the opposite direction (left).

```{r}
ggplot(aids %>% 
         filter(censor == 0) %>% 
         mutate(tx=ifelse(tx == 0, "Control", "IDV")), aes(x = time_d)) + 
  geom_histogram(bins = 20) + 
  facet_grid(.~tx) + 
  ggtitle("Incomplete Observations: Treatment vs. Time to Event") + 
  xlab("Time until End of Study")
```

## Prior ZDV on Complete Observations

We were also curious about the relationship between time to diagnosis/death and number of months of prior ZDV use for non-censored participants. Interestingly, there appeared to be no relationship whatsoever, as evidenced by the following scatterplot:

```{r}
ggplot(non_censored, aes(x = time, y= priorzdv)) + 
  geom_point() + 
  ggtitle("# Months Prior ZDV Treatment vs. Time to Death/Diagnosis")
```

This finding is made even clearer when we log the number of months of prior zdv:

```{r}
ggplot(non_censored, aes(x = time, y= log(priorzdv))) + 
  geom_point() + 
  ggtitle("Log of # Months Prior ZDV Treatment vs. Time to Death/Diagnosis")
```

## Plotting Survival Curves
```{r}
surv <- survfit(Surv(time, censor) ~ 1, 
               data = data, 
               type = "kaplan-meier", 
               conf.typ ="log-log", 
               se.fit = TRUE)
#plot KM curve
ggsurvplot(surv, data = data, 
           risk.table = TRUE, 
           conf.int = TRUE, 
           ggtheme = theme_minimal(), 
           risk.table.y.text.col = T, 
           risk.table.y.text = F)
```

We first fit a survival curve of just the time to death variable. We observe that the overall survival probability of our sample remains relatively high over time and that the last observation is censored. Because treatment is the clinical variable of interest, we next want to see how the survival curves differ between the two treatment groups.

```{r}
surv2 <- survfit(Surv(time, censor) ~ tx, 
               data = data, 
               type = "kaplan-meier", 
               conf.typ ="log-log", 
               se.fit = TRUE)

#plot KM curve
ggsurvplot(surv2, data = data, 
           risk.table = TRUE, 
           conf.int = TRUE, 
           ggtheme = theme_minimal(), 
           risk.table.y.text.col = T, 
           risk.table.y.text = F)

#perform log-rank test
survdiff(Surv(time, censor) ~ tx, data = data, rho = 1)
```

We see that the treatment group for which IDV was also administered has a higher survival probability over time compared to the control group. Performing the log-rank test to test our null hypothesis of whether $S_0(t) = S_1(t)$ for all t results in a $\chi^2_1 = 9.2$ with a p-value of 0.002. We thus reject the null hypothesis and conclude that there is evidence that supports that the survival probabilities are significantly different between the treatment groups over some time intervals. 

# Cox Proportional Hazards Model

## Choosing the Number of Parameters

Our goal is to develop a multivariable survival model for time until death (or diagnosis). In particular, our objective is to build the best predictive model, i.e. we want the highest accuracy on new data. There are 69 deaths (or diagnoses) among 782 patients. The first thing we want to assess is a full additive model. Thus, categorical predictors were expanded using dummy variables. We chose not to include `txgrp` and `strat2` because they were derived (and thus highly correlated with) from other predictor variables. 

First, we make sure that the technical condition for proportional hazards is met with the hypothesis test below. Since no p-values are significant at the $\alpha = 0.05$ level, we next build the full model.

```{r, warning=FALSE}
cox.zph(coxph(Surv(time,censor) ~ tx + sex + raceth + ivdrug + hemophil + karnof + cd4 + priorzdv + age, data=data))
```

```{r, warning=FALSE}
options(scipen = 999)
fit <- coxph(Surv(time, censor) ~ tx + sex + raceth + ivdrug + hemophil + karnof + cd4 + priorzdv + age, data = data)
fit %>% tidy()
```

We also checked whether any of the covariates were multicollinear with each other, and we see that none of them are multicollinear. 

```{r}
#check multicollinearity
vif(fit)
```

We observe that none of the variables have a particularly high variance inflation factor (VIF). While `karnof80` and `karnof90` have relatively higher VIFs, we're not too concerned because they are dummy variables which necessarily have high VIFs due to the smaller proportion of cases in our reference category, `karnof70`. 

The likelihood ratio $\chi^2$ statistic is 91.05 with 21 d.f. After considering whether variables can be mutating into new variables based on our conventional knowledge, and thinking of none in the moment, we decided to try shrinkage to reduce our dimensionality. Here, we're using a lasso penalty Cox PH regression model to select our most important features.

```{r}
set.seed(47)
#initialize covariate matrix
x <- model.matrix(Surv(time, censor) ~ tx + sex + raceth + ivdrug + hemophil + karnof + cd4 + priorzdv + age, data)

#cross validate lambda
cv.fit <- cv.glmnet(x, Surv(data$time, data$censor), family = "cox", maxit = 1000)

#plot cross-validated lambdas
plot(cv.fit)

lassofit <- glmnet(x, Surv(data$time, data$censor), family = "cox", maxit = 1000)

#see which coefficients were kept
active.coefs <- predict(lassofit, type = 'coefficients', s = cv.fit$lambda.min)
```

\begin{table}[ht]
\centering
\begin{tabular}{rr}
  \hline
  Variable & Lasso-ed $\beta$ \\ 
  \hline
(Intercept) & 0.00 \\ 
  tx1 & -0.58 \\ 
  sex2 & 0.13 \\ 
  raceth2 & -0.07 \\ 
  raceth3 & 0.00* \\ 
  raceth4 & 0.64 \\ 
  raceth5 & 0.00* \\ 
  ivdrug2 & -0.06 \\ 
  ivdrug3 & -0.24 \\ 
  hemophil1 & 0.04 \\ 
  karnof80 & 0.00* \\ 
  karnof90 & -0.62 \\ 
  karnof100 & -0.98 \\ 
  cd4 & -0.01 \\ 
  priorzdv & 0.00* \\ 
  age & 0.01 \\ 
   \hline
\end{tabular}
\end{table}

We see that the dummy variable for Hispanic and American Indian, the dummy variable for a Karnofsky score of 80 and priorzdv were shrunk to 0. If we rerun our Cox PH model without priorzdv, which from our EDA was found to not be highly correlated with time, and conduct a likelihood ratio test, let's see what happens.

```{r, warning=FALSE}
fit2 <- coxph(Surv(time, censor) ~ tx + sex + raceth + ivdrug + hemophil + karnof + cd4 + age, data = data)
anova(fit, fit2)
```

Based on our likelihood ratio test results of a $\chi^2_1 = 0.2086$ and a p-value of 0.6479, priorzdv is not needed in the model. 

We do think our model can be more parsimonious; however, so as to avoid overspecification, we look back at the Wald's p-values of the full additive model, and we see that treatment, karnof, cd4, and age (slightly above 0.05) are statistically significant. Before proceeding, we acknowledge that there is a fine line between trying not to overspecify our model and missing potential predictor variables that can contribute some explanatory power. Let's fit a model with only those 4 variables and conduct a likelihood ratio test between this model and the additive model without `priorzdv`.

```{r}
fit3 <- coxph(Surv(time, censor) ~ tx + karnof + cd4 + age, data = data)
anova(fit2, fit3)
```

With a $\chi^2_8 = 6.72$ and a p-value of 0.5667, we conclude that none of the other variables in the additive model were needed. 

Because age had a borderline p-value, let's try removing it from the model and seeing whether it's important or not.

```{r, warning=FALSE}
fit4 <- coxph(Surv(time, censor) ~ tx + karnof + cd4, data = data)
anova(fit3, fit4)
```

It turns out, with a $\chi^2_1 = 2.23$ and a p-value of 0.1353, that age is not needed in the model.

One nagging thought is that marginal variables might have *some* real predictive value even if it's slight. To that end, let's test whether interactions are significant or not. Specifically, because we have reason to believe that there my be interacting effects with treatment group (the clinical variable of interest), let's interact treatment with our categorical covariates along with adjusting for cd4, priorzdv, and age.

```{r, warning=FALSE}
fit.int <- coxph(Surv(time, censor) ~ tx*sex + tx*raceth + tx*ivdrug + tx*hemophil + tx*karnof + cd4 + priorzdv + age, data = data)
anova(fit, fit.int)
```

As we might suspect, none of the interaction terms are needed, so to avoid overfitting, we won't include the interaction terms in our final model. 

## Influential Observations

In brief, an influential observation is one that is an outlier (unusual time to failure given covariates) and has leverage (an unusual observation in the x-direction). This has the effect of strongly influencing $\beta$. To check influence, I'm using dfbeta values which measures the change in $\beta$ when a purported influential point is removed. 

```{r}
#plot dfbeta plot
ggcoxdiagnostics(fit4, type = "dfbeta",
                 linear.predictions = FALSE, ggtheme = theme_bw())

#identify influential points according to dfbeta
dfbeta <- residuals(fit4, type="dfbeta")

#check cd4 influential points
data[dfbeta[,5] > 0.0005,]
```

The above index plots show that comparing the magnitudes of the largest dfbeta values to the regression coefficients suggests that none of the observations is super influential, even though some of the dfbeta values for `cd4` and `tx` are large compared with the others. Generally, we should be careful removing influential observations and throwing away data unless there's a *clear* reason we should (e.g. poor data entry), which we didn't find when checking the influential points for `cd4`. If we look at the deviance residuals for outliers, we might initially be concerned because the distribution does not seem symmetric, with many patients having negative residuals that mean they "lived too long". 

```{r}
ggcoxdiagnostics(fit4, type = "deviance",
                 linear.predictions = FALSE, ggtheme = theme_bw())
```

But then, we remember that our data was imbalanced between those who lived and died to begin with! The majority of patients lived until the last time point. 

## Checking the Log-Linearity Assumption
In class, we investigated the log-linearity assumption which basically verifies whether a continuous predictor variable is linearly correlated with our log hazard ratio. To do this, we first assume our continuous predictor is categorical, and then we take ratios between the instantaneous relative risks. If the ratios are all roughly the same, then we can assume a linear relationship. In our final model, the only continuous predictor we have is `cd4`, so lets categorize it in the following way.

\begin{table}[ht]
\centering
\begin{tabular}{rr}
  \hline
  Level & Desc \\ 
  \hline
  0-70 & low (0) \\ 
  71-140 & low-medium (1) \\
  141-210 & medium (2) \\ 
  211-280 & medium-high (3) \\ 
  281-350 & high (4) \\
   \hline
\end{tabular}
\end{table}

```{r, warning=FALSE}
#turn cd4 into categorical
data2 <- data %>%
  mutate(cd4_group = ifelse(cd4 <= 70, 0,
                ifelse(cd4 > 71 & cd4 <= 140, 1,
                       ifelse(cd4 > 141 & cd4 <= 210, 2, 
                              ifelse(cd4 > 211 & cd4 <= 280, 3,4))))) %>%
  mutate(cd4_group = as.factor(cd4_group))

#fit our model with the categorical cd4 instead of the continuous one
fit5 <- coxph(Surv(time, censor) ~ tx + karnof + cd4_group, data = data2)

fit5 %>% tidy()
```

Now we do some calculations. We want to see whether $ln(\frac{h_{cd4+100}(t)}{h_{cd4}(t)}) = 70\beta$ turns out to be relative constant. Between group0 and group1 of cd4, the log hazard ratio is $ln(\frac{e^{-1.49}}{e^0}) = -1.49$. Between group1 and group2 of cd4, the log hazard ratio is $\frac{e^{-2.84}}{e^{-1.49}} = -1.35$. We can already see that the coefficients for group3 and group4 are whack, which is probably due to a small number of observations within those categories. I would conclude from the log hazard ratios calculated above (they're relatively similar) that it's probably safe to say that `cd4` can be modeled as a continuous predictor. 

## Interpreting our final model
The final model obtained after selecting features via LASSO was $$ln(\frac{h_i(t)}{h_0(t)}) = \beta_1tx1 + \gamma_1karnof80 + \gamma_2karnof90 + \gamma_3karnof100 + \theta_1 cd4$$

#Bootstrapping (Lathan)

##Challenges
Personally, my biggest challenge when learning something new is deciding to what degree I'd like to understand the topic. There is a surface understanding of the definition, a more difficult understanding of the mathematics, and an even more difficult understanding of the conceptual applications. In the case of bootstrap, I think I will find challenging understanding the math behind how bootstrap works.

##A brief overview of Bootstrap and its applications to survival analysis
Bootstrap relies on sampling with replacement of the sample data and in the case of modelling, it is used to evaluate the performance of the model on the original sample. The estimate of the likely performance of the final model on future data is estimated by the average of all the indices computed on the original sample. If we had an original sample of $n$ elements,$X$, we resample $X$ $m$ times to get new bootstrap samples ${X_i,...X_m}$ each with size $n$, derive a model in the bootstrap sample, and apply it to the original sample.

Bootstrapping validates the *process* of obtaining our original Cox PH model. It also tends to provide good estimates of the future performance of our final model if the same modeling process was used in our bootstrap samples. One of the strengths of bootstrapping is thati can estimate the bias due to overfitting in our final model - let's call this quantity "optimism". You can subtract from the original sample estimate the "optimism" to get the bias-corrected estimate of predictive accuracy.

```{r}
#add data to model fit so bootstrap can re-sample
final.fit <- cph(Surv(time, censor) ~ tx + karnof + cd4, data = data)
g <- update(final.fit, x = TRUE, y = TRUE)
set.seed(47)

#bootstrap validation
validate(g, B = 300)
```

Training here is defined as the accuracy when evaluated on the bootstrap sample and test is when the model is applied to the original sample. Our $D_{xy}$ is 0.5632 which is the difference between the probability of concordance and the probability of discordance of pairs of predicted survival times and pairs of observed survival times. This is essentially a measure of our accuracy of our model on new data.

Let's compare the standard errors between our bootstrap coefficients and our likelihood coefficients from our cox PH model. 
```{r}
#initialize parameters
B <- 300
n <- nrow(data)

#initialize matrix
bootse <- matrix(NA, nrow = 1, ncol = 5)

#bootstrap model and obtain SE for each coefficient
set.seed(47)
for(i in 1:B) {
  j <- sample(1:n, n, TRUE)
  bootfit <- update(final.fit, data=data, subset=j)
  bootse <- rbind(bootse, as.vector(sqrt(diag(bootfit$var))))
}

#remove first row
bootse <- bootse[-1,]

#compute avg standard errors -- NOT SURE ABOUT THIS STEP
bootavgse <- colMeans(bootse, 1)
```

```{r}
likese <- fit4 %>% tidy() %>% pull(3)
tibble(likese, bootavgse, coef = c("tx1", "karnof80", "karnof90", "karnof100", "cd4"))
```

They are pretty similar!

## Resources
\begin{itemize}
\item https://statisticalhorizons.com/multicollinearity (Great article on multicollinearity)
\item Harrell, F. (2015) Regression Modeling Strategies. (great textbook on all things survival analysis)
\item https://www.datacamp.com/community/tutorials/bootstrap-r (Overview of bootstrapping)
\item https://stats.stackexchange.com/questions/22017/sample-size-and-cross-validation-methods-for-cox-regression-predictive-models
\end{itemize}

# Gradient Boosted Trees (Madison)

## Background

Gradient boosting machines have gained traction in recent years, popular among Kagglers, researchers, and industry professionals alike. One of the publically availabe algorithms that has fueled this trend is XGBoost (eXtreme Gradient Boosting) developed by Tianqi Chen. XGBoost claims to be a scalable, high-performing, and one of the most computationally efficient implementations of gradient boosting machines out there. It can be used for a variety of regression, classification, and ranking problems.

Gradient boosting is a supervised ensemble method which agglomerates simple, "weak" learners into a more complex whole. In boosting (also called additive training), we start with a constant prediction and iteratively add new functions on top, fixing what we have learned and adding one new model at a time will holding onto functions learned in previous rounds. We fit each model to new residuals based on the previous prediction, then minimize the loss with the addition of the latest prediction. Thus, the residuals from each previous round are used to train the model. In doing this, we are actually updating our model each time using gradient descent - hence the name "gradient" boosting! Gradient boosting is possible with almost any simple classifier, but XGBoost in particular uses an ensemble of decision trees. The objective function within XGBoost also incorporates a regularization term.

By default, XGBoost in Python has mean squared error as its loss funciton within its objective function. However, the creators of XGBoost recently added the option to instead use the Cox regression loss function for right-censored survival time data, and that is what I will be doing. Predictions are then returned on the hazard ratio scale. The package also includes the negative partial log-likelihood for Cox proportional hazards regression as an evaluation metric. 

After I have a good model, I'll produce visualizations of feature importance using SHAP values. I saw these in passing during my internship at Civis last summer, but never got to work with them directly and haven't yet taken the time to fully understand them. 

I will be using the Python `xgboost` package to give myself the added challenge of incorporating both R and Python within one RMarkdown.

My hope is that, in building model for this AIDs survival analysis task, I will come to better understand Gradient Boosting, SHAP values, and survival analysis itself.

## Challenges

I have yet to fully understand what SHAP values are and how to interpret them - will have to read up on that! I'd also like to better understand all the math going on under the surface when using Cox PH within XGBoost. 

## Resources
\begin{itemize}
\item https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf
\item https://xgboost.readthedocs.io/en/latest/tutorials/model.html
\item https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html
\end{itemize}

# Things left to do 
\begin{enumerate}
\item Interpret model
\item Train XGBoost and compare standard Cox model with XGBoost model.
\end{enumerate}
