---
title: "CoxPH"
author: "Lathan Liou"
date: "4/13/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(survival)
library(survminer)
library(rms)
library(glmnet)
library(broom)
data <- read_csv("AIDSdata.csv")

#parse as factor
data <- data %>%
  mutate(tx = as.factor(tx),
         txgrp = as.factor(txgrp),
         strat2 = as.factor(strat2),
         sex = as.factor(sex),
         raceth = as.factor(raceth),
         ivdrug = as.factor(ivdrug),
         hemophil = as.factor(hemophil),
         karnof = as.factor(karnof))
```

# Choosing the Number of Parameters
Our goal is to develop a multivariable survival model for time until death (or diagnosis). There are 69 deaths (or diagnoses) among 782 patients. The first thing I want to assess is a full additive model. Thus, categorical predictors were expanded using dummy variables. I chose not to include `txgrp` and `strat2` because they were found to be highly correlated with other predictor variables from our EDA. I also checked whether any of the covariates were multicollinear with each other, and we see that none of the multicollinear. 

```{r}
options(scipen = 999)
fit <- coxph(Surv(time, censor) ~ tx + sex + raceth + ivdrug + hemophil + karnof + cd4 + priorzdv + age, data = data)
fit %>% tidy()

#check multicollinearity
vif(fit)
```

The likelihood ratio $\chi^2$ statistic is 91.05 with 21 d.f. After considering whether variables can be mutating into new variables based on our conventional knowledge, and thinking of none in the moment, I decided to try shrinkage to reduce our dimensionality. Here, I'm using a lasso penalty Cox PH regression model to select our most important features.

Also, we should note that none of the variables have a particularly high variance inflation factor (VIF). While `karnof80` and `karnof90` have VIFs, I'm not too concerned because they are dummy variables which necessarily have high VIFs due to the smaller proportion of cases in our reference category, `karnof70`. 

```{r}
set.seed(47)
#initialize covariate matrix
x <- model.matrix(Surv(time, censor) ~ tx + sex + raceth + ivdrug + hemophil + karnof + cd4 + priorzdv + age, data)

#cross validate lambda
cv.fit <- cv.glmnet(x, Surv(data$time, data$censor), family = "cox", maxit = 1000)

#plot cross-validated lambdas
plot(cv.fit)

lassofit <- glmnet(x, Surv(data$time, data$censor), family = "cox", maxit = 1000)

#see which coefficients were kept
active.coefs <- predict(lassofit, type = 'coefficients', s = cv.fit$lambda.min)
active.coefs
```

We see that the dummy variable for American Indian, the dummy variable for a Karnofsky score of 80 and priorzdv were shrunk to 0. If we rerun our Cox PH model without priorzdv, which from our EDA was found to not be highly correlated with time, and conduct a likelihood ratio test, let's see what happens.

```{r}
fit2 <- coxph(Surv(time, censor) ~ tx + sex + raceth + ivdrug + hemophil + karnof + cd4 + age, data = data)
anova(fit, fit2)
```

Based on our likelihood ratio test results of a $\chi^2_1 = 0.2086$ and a p-value of 0.6479, priorzdv is not needed in the model. 

I do think our model can be more parsimonious; however, so in order to avoid overspecification, I look back at the Wald's p-values of the full additive model, and I see that treatment, karnof, cd4, and age (slightly above 0.05) are statistically significant. Before proceeding, I will acknowledge here that there is a fine line between trying not to overspecify and introducing bias by adding a "parsimony bias" in our model. Let's fit a model with only those 4 variables and conduct a likelihood ratio test between this model and the additive model without `priorzdv`.

```{r}
fit3 <- coxph(Surv(time, censor) ~ tx + karnof + cd4 + age, data = data)
anova(fit2, fit3)
```

With a $\chi^2_8 = 6.72$ and a p-value of 0.5667, we conclude that none of the other variables in the additive model were needed. 

Because age had a borderline p-value, let's try removing it from the model and seeing whether it's important or not.

```{r}
fit4 <- coxph(Surv(time, censor) ~ tx + karnof + cd4, data = data)
anova(fit3, fit4)
```

It turns out, with a $\chi^2_1 = 2.23$ and a p-value of 0.1353, that age is not needed in the model. 

If our goal is choosing a model that *best describes the data*, I would go with the parsimonious one. However, if we're trying to win the best prediction prize, it might be better to leave all the variables in except for the multicollinear ones as that will inflate standard errors without necessarily improving prediction. The rationale here is that the marginal variables will have *some* real predictive value even if it's slight. To that end, let's test whether interactions are significant or not. Specifically, because we have reason to believe that there my be interacting effects with treatment group (the clinical variable of interest), let's interact treatment with our categorical covariates along with adjusting for cd4, priorzdv, and age.

```{r}
fit.int <- coxph(Surv(time, censor) ~ tx*sex + tx*raceth + tx*ivdrug + tx*hemophil + tx*karnof + cd4 + priorzdv + age, data = data)
fit.int
```

Now, let's consider whether the interaction variables are needed. 

```{r}
anova(fit, fit.int)
```

As we might suspect, none of the interaction terms are needed, so to avoid overfitting, we won't include the interaction terms in our final model. 

#Interpreting the Model

#Check Influential observations, Log Linearity
Let's try looking at the deviance residuals for outliers. 

```{r}
ggcoxdiagnostics(fit4, type = "deviance",
                 linear.predictions = FALSE, ggtheme = theme_bw())

#identify influential points according to deviance
data2 <- data %>%
  mutate(dresids = residuals(fit4, type = "deviance"))
data2 %>%
  filter(dresids > 1)
```
This plot looks concerning, as we see a nonsymmetrical distribution of deviance residuals around 0. Looking more closely at the outliers, we see now that there are some questionable data. For example, patient 589 has a cd4 count of 0, which according to some literature, warrants a second look (could've been faulty machinery).

What if we removed outliers and retrained the model, going through the exact same process?
```{r}
#filter data
data3 <- data2 %>%
  filter(!dresids > 1)

#full additive model
fit_new <- coxph(Surv(time, censor) ~ tx + sex + raceth + ivdrug + hemophil + karnof + cd4 + priorzdv + age, data = data3)

#check multicollinearity
vif(fit_new)

set.seed(47)
#initialize covariate matrix
xnew <- model.matrix(Surv(time, censor) ~ tx + ivdrug + karnof, data3)

#cross validate lambda
cv.fit.new <- cv.glmnet(xnew, Surv(data3$time, data3$censor), family = "cox", maxit = 1000)

#plot cross-validated lambdas
plot(cv.fit.new)

lassofit.new <- glmnet(xnew, Surv(data3$time, data3$censor), family = "cox", maxit = 1000)

#see which coefficients were kept
active.coefs.new <- predict(lassofit.new, type = 'coefficients', s = cv.fit.new$lambda.min)

fit_new2 <- coxph(Surv(time, censor) ~ tx + karnof + cd4 + age, data = data3)
anova(fit_new, fit_new2)
```

## Log Linearity
In class we talked about the linearity assumption of continuous variables and how we could check it by taking the ratios of the relative risks and seeing if they are roughly the same.

```{r}
ggcoxfunctional(Surv(time, status) ~ age + log(age) + sqrt(age), data = lung)
```

#Sources
https://statisticalhorizons.com/multicollinearity (Great article on multicollinearity)
Harrell, F. (2015) Regression Modeling Strategies. (life-saving textbook on all things survival analysis)